#!/usr/bin/env python3
"""
Real AI Verifier - Enhanced Data Tool v2.0
Verificador REAL usando Ollama LLM (no simulado)
"""

import requests
import json
import pandas as pd
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class AIValidationResult:
    """Resultado real de validaci√≥n AI"""
    relationship: str
    is_valid: bool
    confidence_score: float
    explanation: str
    suggested_cardinality: str
    reasoning_steps: List[str]
    llm_raw_response: str
    validation_timestamp: str

class RealAIVerifier:
    """
    Verificador AI REAL usando Ollama
    Hace llamadas reales al LLM para validar relaciones
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434", 
                 model_name: str = "llama3.2:3b", timeout: int = 60):
        self.ollama_url = ollama_url
        self.model_name = model_name
        self.timeout = timeout
        self.available_models = []
        
        # Verificar conexi√≥n real
        self._check_ollama_connection()
    
    def _check_ollama_connection(self) -> bool:
        """Verifica conexi√≥n REAL con Ollama"""
        try:
            print("üîç Verificando conexi√≥n con Ollama...")
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            
            if response.status_code == 200:
                models_data = response.json()
                self.available_models = [model['name'] for model in models_data.get('models', [])]
                
                print(f"‚úÖ Ollama conectado exitosamente")
                print(f"ü§ñ Modelos disponibles: {self.available_models}")
                
                # Verificar si el modelo deseado est√° disponible
                if self.model_name not in self.available_models:
                    if self.available_models:
                        print(f"‚ö†Ô∏è  Modelo {self.model_name} no encontrado")
                        self.model_name = self.available_models[0]
                        print(f"‚úÖ Usando: {self.model_name}")
                    else:
                        raise Exception("No hay modelos instalados en Ollama")
                
                return True
            else:
                raise Exception(f"HTTP {response.status_code}")
                
        except Exception as e:
            print(f"‚ùå Error conectando a Ollama: {e}")
            print("üí° Soluciones:")
            print("   1. Instalar Ollama: https://ollama.ai")
            print("   2. Iniciar servidor: ollama serve")
            print("   3. Instalar modelo: ollama pull llama3.2:3b")
            raise Exception(f"Ollama no disponible: {e}")
    
    def validate_relationships_with_real_ai(self, relationships: List[Dict], 
                                          csv_data: Dict[str, pd.DataFrame],
                                          max_validations: int = 5) -> List[AIValidationResult]:
        """
        Valida relaciones usando LLM REAL
        """
        print(f"\nü§ñ VALIDACI√ìN AI REAL - Modelo: {self.model_name}")
        print("=" * 60)
        
        results = []
        
        # Tomar las mejores relaciones para validar
        sorted_relationships = sorted(relationships, 
                                    key=lambda x: x.get('confidence', 0), 
                                    reverse=True)
        
        for i, relationship in enumerate(sorted_relationships[:max_validations]):
            print(f"\n[{i+1}/{max_validations}] Validando con AI:")
            print(f"   {relationship['relation']}")
            
            try:
                # Llamada REAL al LLM
                ai_result = self._validate_single_relationship_real(relationship, csv_data)
                results.append(ai_result)
                
                # Mostrar resultado
                status = "‚úÖ V√ÅLIDA" if ai_result.is_valid else "‚ùå INV√ÅLIDA"
                print(f"   Resultado: {status} (Confianza AI: {ai_result.confidence_score:.1f}%)")
                print(f"   Explicaci√≥n: {ai_result.explanation}")
                
                # Pausa entre llamadas para no sobrecargar
                if i < max_validations - 1:
                    time.sleep(1)
                
            except Exception as e:
                print(f"   ‚ùå Error con AI: {e}")
                # Crear resultado de error
                error_result = AIValidationResult(
                    relationship=relationship['relation'],
                    is_valid=False,
                    confidence_score=0.0,
                    explanation=f"Error en validaci√≥n AI: {str(e)}",
                    suggested_cardinality="unknown",
                    reasoning_steps=[f"Error t√©cnico: {str(e)}"],
                    llm_raw_response="",
                    validation_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                )
                results.append(error_result)
        
        # Resumen de validaci√≥n AI
        valid_count = len([r for r in results if r.is_valid])
        print(f"\nü§ñ RESUMEN VALIDACI√ìN AI:")
        print(f"   Total validadas: {len(results)}")
        print(f"   ‚úÖ Confirmadas: {valid_count}")
        print(f"   ‚ùå Rechazadas: {len(results) - valid_count}")
        
        return results
    
    def _validate_single_relationship_real(self, relationship: Dict, 
                                         csv_data: Dict[str, pd.DataFrame]) -> AIValidationResult:
        """Valida UNA relaci√≥n usando LLM REAL"""
        
        # Preparar contexto para el LLM
        context = self._prepare_context_for_llm(relationship, csv_data)
        
        # Crear prompt especializado
        prompt = self._create_relationship_validation_prompt(context)
        
        # Llamada REAL a Ollama
        llm_response = self._call_ollama_real(prompt)
        
        # Parsear respuesta del LLM
        validation_result = self._parse_llm_response_real(llm_response, relationship)
        
        return validation_result
    
    def _prepare_context_for_llm(self, relationship: Dict, 
                                csv_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """Prepara contexto rico para el LLM"""
        
        # Extraer informaci√≥n de la relaci√≥n
        relation_parts = relationship['relation'].split(' ‚Üí ')
        from_table_col = relation_parts[0].split('.')
        to_table_col = relation_parts[1].split('.')
        
        from_table, from_col = from_table_col[0], from_table_col[1]
        to_table, to_col = to_table_col[0], to_table_col[1]
        
        # Obtener datos reales
        from_df = csv_data[from_table]
        to_df = csv_data[to_table]
        
        # An√°lisis de datos
        from_values = from_df[from_col].dropna()
        to_values = to_df[to_col].dropna()
        
        # Muestras de datos (sin problemas de serializaci√≥n)
        from_sample = from_values.head(5).astype(str).tolist()
        to_sample = to_values.head(5).astype(str).tolist()
        
        # Intersecci√≥n
        intersection = set(from_values.astype(str)).intersection(set(to_values.astype(str)))
        shared_sample = list(intersection)[:5]
        
        # Estad√≠sticas
        context = {
            'relationship': relationship['relation'],
            'confidence': relationship.get('confidence', 0),
            'overlap_ratio': relationship.get('overlap_ratio', 0),
            'from_table_info': {
                'name': from_table,
                'column': from_col,
                'total_rows': len(from_df),
                'unique_values': len(from_values.unique()),
                'sample_values': from_sample,
                'data_type': str(from_values.dtype)
            },
            'to_table_info': {
                'name': to_table,
                'column': to_col,
                'total_rows': len(to_df),
                'unique_values': len(to_values.unique()),
                'sample_values': to_sample,
                'data_type': str(to_values.dtype)
            },
            'relationship_evidence': {
                'shared_values_count': len(intersection),
                'shared_values_sample': shared_sample,
                'overlap_percentage': len(intersection) / len(from_values.unique()) * 100 if len(from_values.unique()) > 0 else 0
            }
        }
        
        return context
    
    def _create_relationship_validation_prompt(self, context: Dict[str, Any]) -> str:
        """Crea prompt especializado para validaci√≥n de relaciones"""
        
        relationship = context['relationship']
        from_info = context['from_table_info']
        to_info = context['to_table_info']
        evidence = context['relationship_evidence']
        
        prompt = f"""Eres un experto en bases de datos y an√°lisis de relaciones. Analiza si la siguiente relaci√≥n entre tablas CSV es v√°lida y l√≥gica.

RELACI√ìN PROPUESTA:
{relationship}

INFORMACI√ìN DE TABLAS:
Tabla origen: {from_info['name']}
- Columna: {from_info['column']}
- Tipo: {from_info['data_type']}
- Filas totales: {from_info['total_rows']}
- Valores √∫nicos: {from_info['unique_values']}
- Muestra de valores: {from_info['sample_values']}

Tabla destino: {to_info['name']}
- Columna: {to_info['column']}
- Tipo: {to_info['data_type']}
- Filas totales: {to_info['total_rows']}
- Valores √∫nicos: {to_info['unique_values']}
- Muestra de valores: {to_info['sample_values']}

EVIDENCIA DE RELACI√ìN:
- Valores compartidos: {evidence['shared_values_count']}
- Porcentaje de overlap: {evidence['overlap_percentage']:.1f}%
- Muestra de valores compartidos: {evidence['shared_values_sample']}
- Confianza algoritmo: {context['confidence']:.1f}%

INSTRUCCIONES:
Analiza esta relaci√≥n considerando:
1. ¬øLos nombres de columnas sugieren una relaci√≥n l√≥gica?
2. ¬øLos tipos de datos son compatibles?
3. ¬øEl overlap de valores es suficiente y l√≥gico?
4. ¬øLa cardinalidad tiene sentido (muchos a uno, uno a muchos, etc.)?
5. ¬øEn qu√© contexto de negocio tendr√≠a sentido esta relaci√≥n?

Responde en formato JSON ESTRICTO:
{{
  "is_valid": true/false,
  "confidence_score": 0-100,
  "explanation": "Explicaci√≥n clara de por qu√© es v√°lida o inv√°lida",
  "suggested_cardinality": "1:1" o "1:N" o "N:1" o "N:M" o "unknown",
  "reasoning_steps": ["paso 1", "paso 2", "paso 3"],
  "business_context": "En qu√© contexto de negocio tiene sentido"
}}

S√© riguroso en tu an√°lisis. Una relaci√≥n es v√°lida solo si hay evidencia clara."""

        return prompt
    
    def _call_ollama_real(self, prompt: str) -> str:
        """Hace llamada REAL a Ollama"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,  # Baja temperatura para respuestas consistentes
                "top_p": 0.9,
                "num_predict": 1000,  # M√°ximo tokens
                "stop": ["Human:", "Assistant:", "\n\n\n"]
            }
        }
        
        try:
            print(f"   ü§ñ Consultando {self.model_name}...")
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result.get('response', '').strip()
                
                if not llm_response:
                    raise Exception("Respuesta vac√≠a del LLM")
                
                return llm_response
            else:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
                
        except requests.exceptions.Timeout:
            raise Exception(f"Timeout despu√©s de {self.timeout}s")
        except requests.exceptions.ConnectionError:
            raise Exception("Error de conexi√≥n con Ollama")
        except Exception as e:
            raise Exception(f"Error llamando a Ollama: {str(e)}")
    
    def _parse_llm_response_real(self, llm_response: str, relationship: Dict) -> AIValidationResult:
        """Parsea respuesta REAL del LLM"""
        
        try:
            # Buscar JSON en la respuesta
            json_start = llm_response.find('{')
            json_end = llm_response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = llm_response[json_start:json_end]
                
                # Limpiar JSON (remover comentarios, etc.)
                json_str = self._clean_json_response(json_str)
                
                parsed = json.loads(json_str)
                
                return AIValidationResult(
                    relationship=relationship['relation'],
                    is_valid=bool(parsed.get('is_valid', False)),
                    confidence_score=float(parsed.get('confidence_score', 0)),
                    explanation=str(parsed.get('explanation', 'Sin explicaci√≥n')),
                    suggested_cardinality=str(parsed.get('suggested_cardinality', 'unknown')),
                    reasoning_steps=list(parsed.get('reasoning_steps', [])),
                    llm_raw_response=llm_response,
                    validation_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
                )
            else:
                # Si no hay JSON v√°lido, analizar texto
                return self._fallback_text_analysis(llm_response, relationship)
                
        except json.JSONDecodeError as e:
            print(f"   ‚ö†Ô∏è  Error parseando JSON: {e}")
            return self._fallback_text_analysis(llm_response, relationship)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error procesando respuesta: {e}")
            return self._fallback_text_analysis(llm_response, relationship)
    
    def _clean_json_response(self, json_str: str) -> str:
        """Limpia respuesta JSON del LLM"""
        # Remover comentarios y texto extra
        lines = json_str.split('\n')
        clean_lines = []
        
        for line in lines:
            # Remover comentarios
            if '//' in line:
                line = line[:line.index('//')]
            clean_lines.append(line)
        
        return '\n'.join(clean_lines)
    
    def _fallback_text_analysis(self, llm_response: str, relationship: Dict) -> AIValidationResult:
        """An√°lisis de fallback cuando no hay JSON v√°lido"""
        
        response_lower = llm_response.lower()
        
        # Buscar indicadores de validez
        positive_indicators = ['v√°lida', 'valid', 'correcto', 'correct', 's√≠', 'yes', 'l√≥gico', 'logical']
        negative_indicators = ['inv√°lida', 'invalid', 'incorrecto', 'incorrect', 'no', 'il√≥gico']
        
        positive_score = sum(1 for indicator in positive_indicators if indicator in response_lower)
        negative_score = sum(1 for indicator in negative_indicators if indicator in response_lower)
        
        is_valid = positive_score > negative_score
        confidence = min(60 + positive_score * 10, 90) if is_valid else max(40 - negative_score * 10, 10)
        
        return AIValidationResult(
            relationship=relationship['relation'],
            is_valid=is_valid,
            confidence_score=float(confidence),
            explanation="An√°lisis basado en respuesta de texto (JSON no v√°lido)",
            suggested_cardinality="unknown",
            reasoning_steps=["An√°lisis de texto de respuesta LLM"],
            llm_raw_response=llm_response,
            validation_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )

def test_real_ai_verifier():
    """Test r√°pido del verificador AI real"""
    
    print("üß™ TESTING VERIFICADOR AI REAL")
    print("=" * 40)
    
    try:
        # Inicializar verificador
        verifier = RealAIVerifier()
        
        # Datos dummy para test
        test_data = {
            'patients': pd.DataFrame({
                'id': [1, 2, 3],
                'name': ['John', 'Jane', 'Bob']
            }),
            'pets': pd.DataFrame({
                'pet_id': [1, 2],
                'patients_id': [1, 2],
                'name': ['Fluffy', 'Rex']
            })
        }
        
        # Relaci√≥n test
        test_relationship = {
            'relation': 'pets.patients_id ‚Üí patients.id',
            'confidence': 95.0,
            'overlap_ratio': 0.85
        }
        
        # Validar con AI real
        result = verifier._validate_single_relationship_real(test_relationship, test_data)
        
        print(f"‚úÖ Test completado:")
        print(f"   Relaci√≥n: {result.relationship}")
        print(f"   V√°lida: {result.is_valid}")
        print(f"   Confianza AI: {result.confidence_score}")
        print(f"   Explicaci√≥n: {result.explanation}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test fall√≥: {e}")
        return False

if __name__ == "__main__":
    test_real_ai_verifier()
