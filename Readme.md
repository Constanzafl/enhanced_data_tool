# ğŸš€ Enhanced Data Tool v2.0

**Herramienta avanzada para detectar relaciones automÃ¡ticamente en bases de datos usando IA, embeddings semÃ¡nticos y LLMs locales**

## âœ¨ CaracterÃ­sticas Principales

- ğŸ” **DetecciÃ³n AutomÃ¡tica de Relaciones**: Identifica Foreign Keys potenciales sin documentaciÃ³n previa
- ğŸ§  **IA SemÃ¡ntica**: Usa embeddings para encontrar relaciones por similitud conceptual
- ğŸ¤– **VerificaciÃ³n LLM**: Valida relaciones usando modelos de lenguaje locales (Ollama)
- ğŸ“ **ExportaciÃ³n DBML**: Genera diagramas ER visuales automÃ¡ticamente
- ğŸ¯ **Multi-estrategia**: Combina anÃ¡lisis de nombres, patrones, datos y semÃ¡ntica
- ğŸ”’ **Privacidad**: Todo ejecuta localmente, sin enviar datos a APIs externas

## ğŸš€ InstalaciÃ³n RÃ¡pida

### OpciÃ³n 1: InstalaciÃ³n AutomÃ¡tica (Recomendada)

```bash
# Descargar e instalar automÃ¡ticamente
python setup_tool.py
```

### OpciÃ³n 2: InstalaciÃ³n Manual

```bash
# 1. Crear entorno virtual
python -m venv venv_data_tool
source venv_data_tool/bin/activate  # Linux/Mac
# o
venv_data_tool\Scripts\activate     # Windows

# 2. Instalar dependencias bÃ¡sicas
pip install -r requirements.txt

# 3. Instalar embeddings (opcional pero recomendado)
python -m pip install --upgrade pip
pip install sentence-transformers scikit-learn

# 4. Configurar Ollama para LLM (opcional)
# Instalar desde: https://ollama.ai
ollama serve
ollama pull llama3.2:3b
```

## ğŸ¯ Uso RÃ¡pido

### EjecuciÃ³n Simple

```python
from complete_data_tool import analyze_database_complete

# AnÃ¡lisis completo de una base de datos
results = analyze_database_complete(
    db_path="mi_database.db",
    output_dir="./output",
    use_embeddings=True,  # DetecciÃ³n semÃ¡ntica
    use_llm=True         # VerificaciÃ³n con IA
)
```

### Uso Avanzado

```python
from complete_data_tool import CompleteDataTool

# ConfiguraciÃ³n personalizada
tool = CompleteDataTool(
    db_path="mi_database.db",
    use_embeddings=True,
    use_llm=True
)

# AnÃ¡lisis paso a paso
schema = tool.extract_schema()
relationships = tool.detect_relationships()
llm_results = tool.verify_with_llm(max_verifications=10)
dbml_content = tool.generate_dbml("mi_esquema.dbml")
```

## ğŸ”§ ConfiguraciÃ³n

### Archivo config.json

```json
{
  "database": {
    "default_type": "sqlite",
    "sample_db": "example_store.db"
  },
  "analysis": {
    "use_embeddings": true,
    "use_llm": true,
    "max_llm_verifications": 5,
    "confidence_threshold": 30
  },
  "ollama": {
    "url": "http://localhost:11434",
    "model": "llama3.2:3b",
    "timeout": 30
  }
}
```

## ğŸ› SoluciÃ³n de Errores Comunes

### Error: "Object of type DataFrame is not JSON serializable"

**Causa**: El LLM verifier original intentaba serializar DataFrames directamente.

**SoluciÃ³n**: âœ… **YA CORREGIDO** en v2.0
- Los DataFrames se convierten a tipos serializables automÃ¡ticamente
- Se implementÃ³ `_serialize_dataframe()` que maneja todos los tipos numpy/pandas

```python
# CÃ³digo corregido internamente:
def _serialize_dataframe(self, df: pd.DataFrame) -> Dict[str, Any]:
    serializable_data = []
    for _, row in df.iterrows():
        row_data = {}
        for col, value in row.items():
            if pd.isna(value):
                row_data[col] = None
            elif isinstance(value, (np.integer, np.int64)):
                row_data[col] = int(value)
            # ... mÃ¡s conversiones
```

### Error: "sentence-transformers not found"

**SoluciÃ³n**:
```bash
pip install sentence-transformers
# o usar solo detecciÃ³n bÃ¡sica:
tool = CompleteDataTool(db_path="db.sqlite", use_embeddings=False)
```

### Error: "Ollama connection failed"

**SoluciÃ³n**:
```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh  # Linux/Mac
# o descargar desde https://ollama.ai para Windows

# 2. Iniciar servidor
ollama serve

# 3. Instalar modelo
ollama pull llama3.2:3b

# 4. Verificar conexiÃ³n
curl http://localhost:11434/api/tags
```

### Error: "No relationships detected"

**Posibles causas y soluciones**:

1. **Umbral de confianza muy alto**:
```python
# Reducir umbral
tool = CompleteDataTool(db_path="db.sqlite")
tool.confidence_threshold = 20  # Default: 30
```

2. **Datos de muestra insuficientes**:
```python
# Aumentar muestra
tool._get_sample_data(table_name, limit=20)  # Default: 10
```

3. **Patrones FK no estÃ¡ndar**:
```python
# Agregar patrones personalizados
tool.fk_patterns.extend([r'.*_foreign$', r'ref_.*'])
```

## ğŸ“Š Niveles de DetecciÃ³n

### 1. BÃ¡sico (Sin dependencias externas)
```python
tool = CompleteDataTool(db_path="db.sqlite", use_embeddings=False, use_llm=False)
```
- âœ… DetecciÃ³n por patrones de nombres
- âœ… ValidaciÃ³n con datos de muestra
- âœ… ExportaciÃ³n DBML
- âš¡ RÃ¡pido y ligero

### 2. Con Embeddings (Recomendado)
```python
tool = CompleteDataTool(db_path="db.sqlite", use_embeddings=True, use_llm=False)
```
- âœ… Todo lo anterior +
- ğŸ§  Similitud semÃ¡ntica
- ğŸ¯ DetecciÃ³n de relaciones conceptuales
- ğŸ“ˆ Mayor precisiÃ³n

### 3. Completo (MÃ¡xima precisiÃ³n)
```python
tool = CompleteDataTool(db_path="db.sqlite", use_embeddings=True, use_llm=True)
```
- âœ… Todo lo anterior +
- ğŸ¤– VerificaciÃ³n con IA
- ğŸ“ Explicaciones detalladas
- ğŸ¯ Cardinalidad sugerida

## ğŸ—„ï¸ Compatibilidad de Bases de Datos

### SQLite (Completamente soportado)
```python
tool = CompleteDataTool("database.sqlite")
```

### PostgreSQL (Experimental)
```python
# Instalar driver
pip install psycopg2-binary

# Configurar conexiÃ³n
import psycopg2
conn = psycopg2.connect(
    host="localhost",
    database="mydb", 
    user="user",
    password="password"
)
```

### MySQL (Experimental)
```python
# Instalar driver
pip install pymysql

# Configurar conexiÃ³n
import pymysql
conn = pymysql.connect(
    host="localhost",
    database="mydb",
    user="user", 
    password="password"
)
```

## ğŸ“ GeneraciÃ³n de Diagramas

### Exportar a DBML
```python
dbml_content = tool.generate_dbml("mi_esquema.dbml", "Mi Proyecto")
```

### Visualizar en dbdiagram.io
1. Abrir el archivo `.dbml` generado
2. Copiar todo el contenido
3. Ir a [dbdiagram.io](https://dbdiagram.io/d)
4. Pegar el cÃ³digo en el editor
5. Â¡Disfrutar el diagrama ER interactivo!

### Ejemplo de DBML generado:
```dbml
// Mi Proyecto - Generado automÃ¡ticamente

Table customers {
  id integer [primary key]
  name varchar [not null]
  email varchar
  created_at timestamp
}

Table orders {
  id integer [primary key]
  customer_id integer [not null]
  total decimal
}

// RelaciÃ³n detectada automÃ¡ticamente
Ref: orders.customer_id > customers.id // Confianza: 95.0% | LLM: âœ… VÃ¡lida
```

## ğŸš€ Ejemplos de Uso

### Ejemplo 1: E-commerce
```python
# Detectar relaciones en BD de e-commerce
results = analyze_database_complete("ecommerce.db")

# Relaciones tÃ­picas detectadas:
# - orders.customer_id â†’ customers.id
# - order_items.order_id â†’ orders.id  
# - order_items.product_id â†’ products.id
# - products.category_id â†’ categories.id
```

### Ejemplo 2: CRM
```python
# AnÃ¡lisis de sistema CRM
tool = CompleteDataTool("crm_system.db", use_embeddings=True)
results = tool.analyze_complete()

# Relaciones tÃ­picas:
# - contacts.company_id â†’ companies.id
# - deals.contact_id â†’ contacts.id
# - activities.deal_id â†’ deals.id
```

### Ejemplo 3: AnÃ¡lisis Masivo
```python
import os
from pathlib import Path

# Analizar mÃºltiples bases de datos
db_files = Path("databases/").glob("*.db")

for db_file in db_files:
    print(f"Analizando {db_file.name}...")
    
    results = analyze_database_complete(
        str(db_file),
        output_dir=f"output/{db_file.stem}",
        use_embeddings=True,
        use_llm=False  # MÃ¡s rÃ¡pido para anÃ¡lisis masivo
    )
    
    print(f"âœ… {results['summary']['detected_relationships']} relaciones detectadas")
```

## ğŸ”¬ Algoritmos de DetecciÃ³n

### 1. AnÃ¡lisis de Patrones
- Patrones FK comunes: `*_id`, `*_key`, `*_fk`, `*_ref`
- Prefijos: `id_*`, `fk_*`, `ref_*`
- Sufijos especiales: `*_foreign`, `*_link`

### 2. Similitud SemÃ¡ntica
- Embeddings de sentence-transformers
- ComparaciÃ³n de contexto tabla-columna
- Mapeo de conceptos relacionados

### 3. ValidaciÃ³n de Datos
- VerificaciÃ³n de integridad referencial
- CÃ¡lculo de ratio de coincidencias
- AnÃ¡lisis de cardinalidad

### 4. VerificaciÃ³n LLM
- AnÃ¡lisis contextual con IA
- ExplicaciÃ³n de decisiones
- Sugerencias de cardinalidad

## ğŸ“ˆ MÃ©tricas de Confianza

### Escala de Confianza
- ğŸŸ¢ **80-100%**: Alta confianza - Muy probable que sea correcta
- ğŸŸ¡ **60-79%**: Confianza media - Revisar manualmente
- ğŸ”´ **30-59%**: Baja confianza - Requiere validaciÃ³n
- âŒ **<30%**: Descartada automÃ¡ticamente

### Factores que Aumentan Confianza
- âœ… Coincidencia exacta de nombres
- âœ… PatrÃ³n FK reconocido
- âœ… Alta similitud semÃ¡ntica
- âœ… ValidaciÃ³n con datos reales
- âœ… ConfirmaciÃ³n por LLM

## ğŸ› ï¸ Desarrollo y ContribuciÃ³n

### Estructura del Proyecto
```
enhanced-data-tool/
â”œâ”€â”€ complete_data_tool.py          # Herramienta principal
â”œâ”€â”€ enhanced_schema_analyzer.py    # Analizador base
â”œâ”€â”€ embedding_relationship_detector.py  # Detector con embeddings
â”œâ”€â”€ robust_llm_verifier.py        # Verificador LLM
â”œâ”€â”€ setup_tool.py                 # Instalador automÃ¡tico
â”œâ”€â”€ requirements.txt              # Dependencias
â”œâ”€â”€ config.json                   # ConfiguraciÃ³n
â”œâ”€â”€ example_usage.py              # Ejemplos
â””â”€â”€ output/                       # Resultados
```

### Contribuir
1. Fork el repositorio
2. Crear rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m 'Agregar nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

### Tests
```bash
# Ejecutar tests
python -m pytest tests/

# Con cobertura
python -m pytest --cov=enhanced_data_tool tests/
```

## ğŸ“ Roadmap

### v2.1 (PrÃ³xima versiÃ³n)
- [ ] Soporte nativo para PostgreSQL/MySQL
- [ ] DetecciÃ³n de relaciones N:M
- [ ] ExportaciÃ³n a SQL DDL
- [ ] Interfaz web bÃ¡sica

### v2.2
- [ ] API REST
- [ ] MÃ¡s formatos de exportaciÃ³n (PlantUML, Mermaid)
- [ ] DetecciÃ³n de Ã­ndices recomendados
- [ ] AnÃ¡lisis de performance

### v3.0 (Futuro)
- [ ] Interfaz grÃ¡fica completa
- [ ] Soporte para NoSQL
- [ ] IntegraciÃ³n con herramientas BI
- [ ] AnÃ¡lisis de calidad de datos

## ğŸ¤ Soporte

### Reportar Bugs
- [GitHub Issues](https://github.com/Constanzafl/data_tool/issues)
- Incluir: versiÃ³n, SO, base de datos, log completo

### Preguntas Frecuentes

**P: Â¿Funciona sin conexiÃ³n a internet?**
R: SÃ­, completamente. Solo la primera instalaciÃ³n de embeddings requiere descarga.

**P: Â¿Es seguro? Â¿Se envÃ­an datos externos?**
R: Totalmente seguro. Todo procesa localmente, incluso el LLM con Ollama.

**P: Â¿QuÃ© tan preciso es?**
R: En tests internos: 85-95% de precisiÃ³n en relaciones obvias, 70-80% en relaciones complejas.

**P: Â¿Funciona con bases de datos grandes?**
R: SÃ­, pero usa muestreo de datos para performance. Configurable en `sample_size`.

## ğŸ“„ Licencia

MIT License - Ver archivo LICENSE para detalles.

## ğŸ™ Agradecimientos

- [sentence-transformers](https://github.com/UKPLab/sentence-transformers) por embeddings semÃ¡nticos
- [Ollama](https://ollama.ai) por LLMs locales
- [dbdiagram.io](https://dbdiagram.io) por visualizaciÃ³n de diagramas
- Comunidad open source por feedback y contribuciones

---

â­ **Â¡Si te gusta el proyecto, dale una estrella en GitHub!**

ğŸš€ **Â¡Comienza ahora!** `python setup_tool.py`
